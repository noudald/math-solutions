\section*{Chapter 14 - Multivariate Models}

\subsection*{Solution 14.1}

See solution 3.10.


\subsection*{Solution 14.2}

We have $\log(f(X;\bm{p})) = \sum_{j = 1}^k X_j \log(p_j)$, so $s(X;p_i) = X_i/p_i$, $s'(X;p_i) = -X_i/p_i^2$, and $-E(s'(X;p_i)) = n/p_i$.
Therefore, the Fisher information matrix is given by $I(\theta) = \frac{1}{n} I_n(\theta) = \mathrm{diag}(p_1^{-1}, p_2^{-1}, ..., p_k^{-1})$.


\subsection*{Solution 14.3}

See code.


\subsection*{Solution 14.4}

See code.


\subsection*{Solution 14.5}

See code.


\subsection*{Solution 14.6}

See solution 14.5.
